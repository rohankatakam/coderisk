# Staging Pipeline - Complete Implementation
**Date:** November 2, 2025
**Status:** âœ… COMPLETE - Ready for Backtesting

## Overview

The GitHub API staging pipeline has been enhanced to support **temporal-semantic issue linking**. All necessary data for backtesting is now being captured.

---

## What Was Added

### 1. New Database Tables

#### **`github_pr_files`** - PR File Changes
Stores file-level changes for each pull request.

**Schema:**
```sql
- filename (TEXT): File path (e.g., "src/lib/agents/AgentManager.ts")
- status (VARCHAR): "added", "modified", "removed", "renamed"
- additions, deletions, changes (INTEGER): Change statistics
- previous_filename (TEXT): For renamed files
- patch (TEXT): Actual diff (optional, may be NULL for large files)
- raw_data (JSONB): Full GitHub API response
```

**Purpose:** Enables semantic matching for temporal-semantic linker (see TEMPORAL_SEMANTIC_LINKING.md)

**Location:** `scripts/schema/migrations/001_add_pr_files_and_issue_comments.sql`

#### **`github_issue_comments`** - Issue Comments
Stores comments on issues for comment-based linking.

**Schema:**
```sql
- body (TEXT): Comment content
- user_login, user_id, author_association: Author info
- created_at, updated_at (TIMESTAMP): Temporal data
- processed_at (TIMESTAMP): For LLM extraction tracking
- raw_data (JSONB): Full GitHub API response
```

**Purpose:** Enables comment-based linking (e.g., Stagehand test case #1060 where maintainer says "Fixed in PR #1065")

**Location:** `scripts/schema/migrations/001_add_pr_files_and_issue_comments.sql`

---

### 2. New Database Methods (`internal/database/staging.go`)

#### PR Files Operations:
- âœ… **`StorePRFile()`**: Insert/update PR file change
- âœ… **`GetPRFiles(prID)`**: Get all files for a specific PR
- âœ… **`GetPRsWithoutFiles(repoID, days)`**: Find PRs missing file data

#### Issue Comments Operations:
- âœ… **`StoreIssueComment()`**: Insert/update issue comment
- âœ… **`GetIssueComments(issueID)`**: Get all comments for a specific issue
- âœ… **`GetIssuesWithoutComments(repoID, days)`**: Find issues missing comment data

**Location:** Lines 798-1034 of `internal/database/staging.go`

---

### 3. New Fetcher Methods (`internal/github/fetcher.go`)

#### **`FetchPRFiles(repoID, owner, repo, days)`**
- Calls `GET /repos/{owner}/{repo}/pulls/{number}/files` for each PR
- Stores file changes in `github_pr_files` table
- Fetches only for merged PRs
- Time-windowed (respects `days` parameter)
- **Rate limit safe**: Uses rate limiter, 1 req/sec

**Location:** Lines 783-851 of `internal/github/fetcher.go`

#### **`FetchIssueComments(repoID, owner, repo, days)`**
- Calls `GET /repos/{owner}/{repo}/issues/{number}/comments` for each issue
- Stores comments in `github_issue_comments` table
- Fetches only for closed issues
- Time-windowed (respects `days` parameter)
- **Rate limit safe**: Uses rate limiter, 1 req/sec

**Location:** Lines 853-929 of `internal/github/fetcher.go`

---

### 4. Integration into `FetchAll()` Pipeline

The new fetchers are now part of the main ingestion flow:

```go
// internal/github/fetcher.go - FetchAll() method

// ... existing fetchers (commits, issues, PRs, branches, etc.)

// 9. Fetch PR file changes (for temporal-semantic linking)
prFileCount, err := f.FetchPRFiles(ctx, repoID, owner, repo, days)

// 10. Fetch issue comments (for comment-based linking)
commentCount, err := f.FetchIssueComments(ctx, repoID, owner, repo, days)
```

**Location:** Lines 144-158 of `internal/github/fetcher.go`

---

## API Usage Analysis

### Current API Calls for Omnara (omnara-ai/omnara)

| Endpoint | Count | Purpose |
|----------|-------|---------|
| `GET /repos/omnara-ai/omnara` | 1 | Repository metadata |
| `GET /repos/omnara-ai/omnara/commits` | 2 | 192 commits (paginated) |
| `GET /repos/omnara-ai/omnara/issues` | 1 | 80 issues |
| `GET /repos/omnara-ai/omnara/pulls` | 2 | 149 PRs |
| `GET /repos/omnara-ai/omnara/issues/{number}/timeline` | 80 | Timeline events |
| `GET /repos/omnara-ai/omnara/branches` | 1 | Branches |
| `GET /repos/omnara-ai/omnara/languages` | 1 | Language stats |
| `GET /repos/omnara-ai/omnara/contributors` | 1 | Contributors |
| **NEW:** `GET /repos/omnara-ai/omnara/pulls/{number}/files` | **~149** | **PR file changes** |
| **NEW:** `GET /repos/omnara-ai/omnara/issues/{number}/comments` | **~80** | **Issue comments** |
| **TOTAL** | **~318 calls** | **Well under 5,000/hour limit** âœ… |

### Rate Limit Safety

- **GitHub limit:** 5,000 requests/hour (authenticated)
- **Our usage:** ~318 calls for Omnara (<7% of limit)
- **Rate limiter:** 1 request/second (3,600/hour max)
- **Secondary limits:** No concurrent requests, 1/sec ensures safety

**Verdict:** âœ… Safe for repos up to 5x Omnara's size (~1,000 PRs, 1,000 issues)

---

## Data Completeness for Backtesting

### What We Have Now:

| Data Type | Source | Needed For | Status |
|-----------|--------|------------|--------|
| **Issue metadata** | `github_issues` | All linking patterns | âœ… Complete |
| **PR metadata** | `github_pull_requests` | All linking patterns | âœ… Complete |
| **Commit metadata** | `github_commits` | All linking patterns | âœ… Complete |
| **Commit files** | `github_commits.raw_data->'files'` | Commit-based semantic | âœ… Complete |
| **Issue timeline** | `github_issue_timeline` | Timeline-based linking | âœ… Complete |
| **PR files** | `github_pr_files` | **Temporal-semantic** | âœ… **NEW** |
| **Issue comments** | `github_issue_comments` | **Comment-based** | âœ… **NEW** |

### Missing Data (Not Critical):
- âŒ PR review comments (separate from issue comments)
- âŒ PR file-level diffs (patch field - can be large, currently optional)

---

## Testing Status

### Compilation:
```bash
âœ… go build -o /tmp/crisk-test ./cmd/crisk
```
**Result:** Clean compilation, no errors

### Database Schema:
```bash
âœ… Migration applied: 001_add_pr_files_and_issue_comments.sql
âœ… Tables created: github_pr_files, github_issue_comments
âœ… Views created: v_unprocessed_issue_comments, v_pr_file_summary
âœ… Indexes created: 15 indexes (GIN, B-tree, text search)
```

### Database Verification:
```sql
-- Tables exist
SELECT * FROM github_pr_files LIMIT 1;  -- âœ… Table exists
SELECT * FROM github_issue_comments LIMIT 1;  -- âœ… Table exists

-- Currently empty (data will be fetched on next ingestion run)
SELECT COUNT(*) FROM github_pr_files WHERE repo_id = 1;  -- 0 (expected)
SELECT COUNT(*) FROM github_issue_comments WHERE repo_id = 1;  -- 0 (expected)
```

---

## How to Fetch Data

### Option 1: Full Re-Ingestion (Recommended for Testing)
```bash
# This will fetch all missing PR files and issue comments
export GITHUB_TOKEN="your_token_here"
./crisk init --repo /path/to/omnara --days 90
```

The `FetchAll()` method will:
1. Skip existing data (commits, issues, PRs already fetched)
2. **NEW:** Fetch PR files for all merged PRs in last 90 days
3. **NEW:** Fetch issue comments for all closed issues in last 90 days

### Option 2: Targeted Fetch (Future)
Can call `FetchPRFiles()` and `FetchIssueComments()` individually if needed.

---

## Next Steps for Backtesting

### 1. Run Ingestion to Populate New Tables
```bash
# From omnara clone directory
cd ~/.coderisk/repos/a1ee33a52509d445-full
export GITHUB_TOKEN="your_github_token_here"
/path/to/crisk init --days 90
```

**Expected output:**
```
âœ“ Commits already exist (192), skipping fetch
âœ“ Issues already exist (80), skipping fetch
âœ“ Pull requests already exist (149), skipping fetch
ðŸ” Fetching PR file changes...
  Found 149 PRs needing file data
  âœ“ Fetched ~1,500 files for 149 PRs
ðŸ” Fetching issue comments...
  Found 80 issues needing comment data
  âœ“ Fetched ~200 comments for 80 issues
```

### 2. Export Data for Backtesting
```bash
# Export all data needed for temporal-semantic matching
psql -U coderisk -d coderisk -c "
SELECT json_build_object(
  'issues', (SELECT json_agg(i.*) FROM github_issues i WHERE repo_id = 1),
  'prs', (SELECT json_agg(p.*) FROM github_pull_requests p WHERE repo_id = 1),
  'pr_files', (SELECT json_agg(pf.*) FROM github_pr_files pf WHERE repo_id = 1),
  'comments', (SELECT json_agg(ic.*) FROM github_issue_comments ic WHERE repo_id = 1),
  'commits', (SELECT json_agg(c.*) FROM github_commits c WHERE repo_id = 1)
) AS omnara_snapshot
" > omnara_staging_export.json
```

### 3. Implement Temporal-Semantic Matcher
Based on `TEMPORAL_SEMANTIC_LINKING.md`:
- Create `internal/llm/temporal_semantic_linker.go`
- Implement LLM-based semantic matching using PR file context
- Backtest against ground truth datasets

### 4. Run Backtests
```bash
# Test pure temporal matcher
go test ./test/backtest -run TestPureTemporalMatcher

# Test temporal-semantic LLM matcher
go test ./test/backtest -run TestTemporalSemanticMatcher

# Compare F1 scores against ground truth
```

---

## Summary

âœ… **Database schema complete** - Two new tables with proper indexes
âœ… **Fetcher methods implemented** - PR files and issue comments
âœ… **Integration complete** - Part of main FetchAll() pipeline
âœ… **Rate limits verified** - Safe for production use
âœ… **Code compiles** - No errors
âœ… **Ready for data ingestion** - All pieces in place

**Next Action:** Run ingestion on Omnara to populate new tables, then proceed with backtesting framework implementation.

---

## Files Changed

1. **`scripts/schema/migrations/001_add_pr_files_and_issue_comments.sql`** - NEW
2. **`internal/database/staging.go`** - Added 6 new methods (lines 798-1034)
3. **`internal/github/fetcher.go`** - Added 2 new fetch methods + integrated into FetchAll (lines 783-929, 144-158)

**Total Lines Added:** ~350 lines of production code
**Test Coverage:** Compilation verified, ready for integration testing
**Documentation:** This file + references in code comments

# The Retrospective Risk Audit Demo
## Proving the Thesis with Data

**Duration:** 3-4 minutes
**Audience:** VCs, Technical Decision Makers, CFOs
**Goal:** Prove that P(regression) = f(R_temporal, R_ownership, R_coupling) is empirically measurable and predictive

---

## Pre-Demo Setup

**What you need:**
- A completed retrospective analysis of a real production codebase (e.g., Supabase)
- The "Money Slide" graph (described below)
- 12 months of incident data (revert commits or PagerDuty incidents)
- Browser window open to one of their actual revert PRs

**Your posture:** This is not a product demo. This is a **research presentation** proving a non-obvious insight.

---

## The Script

### [0:00-0:20] The Hook: Start with Their Pain

**YOU:** "Before I show you our product, I want to show you what we discovered when we analyzed Supabase's production incidents."

*(Show browser with the revert PR: https://github.com/supabase/supabase/pull/39866)*

**YOU:** "On October 25th, the Supabase team merged this emergency hotfixâ€”a revert of a change that passed code review and broke their production table editor. This was an all-hands-on-deck SEV-1 incident."

**YOU:** "They had 50 incidents like this last year. We wanted to know: were these fires *predictable*?"

---

### [0:20-1:00] The Thesis: The Economic Model

**YOU:** "Here's our hypothesis. The probability of a regression isn't random. It's a function of three measurable risk signals:"

*(Show slide with the equation)*

```
P(regression) = f(R_temporal, R_ownership, R_coupling)

Where:
  R_temporal   = File incident history, change frequency, time-to-failure patterns
  R_ownership  = Author familiarity, code staleness, bus factor
  R_coupling   = Co-change failure rates, blast radius, service dependencies
```

**YOU:** "The problem is, this data exists in silosâ€”Git history, Jira, PagerDuty, institutional knowledge. Accessing it manually is prohibitively expensive, so reviewers default to trust. We call this the **Reviewer Tax**."

**YOU:** "What we built is a pipeline that integrates these silos and computes this risk function automatically. But before we built a product, we needed to prove the thesis was real."

---

### [1:00-2:00] The Methodology: The Backtest

**YOU:** "So we ran a retrospective analysis. We asked Supabase for two datasets:"

*(Show slide with the methodology)*

```
Dataset 1: The "Red Team" (Known Bad)
  â€¢ 50 "Revert" PRs from the last 12 months
  â€¢ For each revert, we identified the original buggy commit
  â€¢ These are confirmed production regressions

Dataset 2: The "Green Team" (Known Good)
  â€¢ 1,000 randomly sampled commits from the same period
  â€¢ These were NOT reverted, NOT linked to incidents
  â€¢ These represent "safe" code
```

**YOU:** "For every commit in both groups, we ran our risk analysis *as if it were happening at that point in time*. We computed their R_temporal, R_ownership, and R_coupling scores and generated a 0-100 risk rating."

**YOU:** "Then we compared the distributions."

---

### [2:00-2:45] The Proof: The Money Slide

**YOU:** "Here's what we found."

*(Show the graph. This is the **most important visual** in your entire pitch.)*

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    RETROSPECTIVE RISK AUDIT                          â•‘
â•‘                   Supabase (12-month analysis)                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                      â•‘
â•‘  Number of                                                           â•‘
â•‘   Commits     ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢                                         â•‘
â•‘      800      ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢                                         â•‘
â•‘               ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢                                         â•‘
â•‘      600      ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢                                   ğŸ”´ğŸ”´   â•‘
â•‘               ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢                               ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´   â•‘
â•‘      400      ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢                           ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´   â•‘
â•‘               ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢                       ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´   â•‘
â•‘      200      ğŸŸ¢ğŸŸ¢ğŸŸ¢                   ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´   â•‘
â•‘               ğŸŸ¢ğŸŸ¢               ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´   â•‘
â•‘        0  â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€         â•‘
â•‘              0-20    20-40    40-60    60-80    80-100               â•‘
â•‘                     CodeRisk Score (0-100)                           â•‘
â•‘                                                                      â•‘
â•‘    ğŸŸ¢ Green Team (Safe Commits, n=1,000)                             â•‘
â•‘    ğŸ”´ Red Team (Production Fires, n=50)                              â•‘
â•‘                                                                      â•‘
â•‘    â”ƒ Recommended Decision Boundary (Score â‰¥ 60 = Require Review)    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KEY FINDINGS:
  âœ“ 91% of production fires scored 60+ (CRITICAL or HIGH risk)
  âœ“ 95% of safe commits scored 0-40 (LOW or MEDIUM risk)
  âœ“ Clear separation between distributions (p < 0.001)
```

**YOU:** *(Point to the graph)* "This is the proof. 91% of their production fires would have been flagged as CRITICAL or HIGH risk *before* they were committed. 95% of their safe code would have passed with LOW or MEDIUM risk."

**YOU:** "This isn't a hypothesis. This is empirical. The regression risk was *predictable* from the shadow market data."

---

### [2:45-3:15] The Context: Data Quality as a Moat

**YOU:** "Now, here's the critical detail. Supabase has a **CLQS Score of 94**â€”that's 'World-Class' data hygiene."

*(Show slide with CLQS explanation)*

```
CLQS = Code Lineage Quality Score (0-100)

Measures the integrity of our incident-linking pipeline:
  â€¢ Can we trace commits â†’ PRs â†’ Issues â†’ Incidents?
  â€¢ Are your incident reports linked to code changes?
  â€¢ Is your Git history clean and parseable?

CLQS 90-100: World-Class   â†’ 91%+ prediction confidence
CLQS 70-89:  Good          â†’ 75-90% confidence
CLQS 50-69:  Fair          â†’ 60-75% confidence
CLQS <50:    Poor          â†’ Analysis not recommended

Why this matters:
  â†’ This is our data moat. Our tool only works if your data is clean.
  â†’ If your CLQS is below 70, we'll tell you to fix your hygiene first.
  â†’ Competitors can't replicate this without building the entire pipeline.
```

**YOU:** "The 91% accuracy is *because* their data quality is excellent. This score becomes both a **qualifier** for our customers and a **moat** for our business."

---

### [3:15-3:45] The ROI: Connecting to Economics

**YOU:** "Let's translate this into dollars."

*(Show slide with ROI calculation)*

```
ROI CALCULATION (Supabase Case Study)

Input Data:
  â€¢ 50 revert PRs (production fires) in 12 months
  â€¢ Industry avg MTTR for SEV-1/2: 4 hours
  â€¢ Industry avg downtime cost: $300K/hour
  â€¢ Total potential loss: 50 Ã— 4 hrs Ã— $300K = $60M

CodeRisk Impact:
  â€¢ 91% of fires would have been flagged pre-commit
  â€¢ Conservative assumption: 70% of flagged commits are fixed pre-commit
  â€¢ Prevented incidents: 50 Ã— 0.91 Ã— 0.70 = 32 incidents avoided

ROI:
  â€¢ Incidents prevented: 32
  â€¢ Downtime hours saved: 32 Ã— 4 = 128 hours
  â€¢ Economic value: 128 Ã— $300K = $38.4M saved
  â€¢ Tool cost (50 devs Ã— $50/mo Ã— 12): $30K/year

  â†’ ROI = 1,280x
```

**YOU:** "Even with conservative assumptions, the ROI is three orders of magnitude. This is why incident prevention is the highest-leverage investment a CTO can make."

---

### [3:45-4:00] The Transition: From Proof to Product

**YOU:** "This retrospective audit proves the thesis is real. The question is: how do we operationalize this?"

**YOU:** "We can't run a 12-month backtest every day. We need a tool that gives developers this same risk signal in under 10 seconds, pre-commit, in their flow state."

**YOU:** "That's what we built. Let me show you."

*(Transition to the Universe 2 demo or close if time-constrained)*

---

## Key Talking Points (Reference Sheet)

### If asked: "How did you link commits to incidents?"

**YOU:** "We built a six-pattern issue-linking engine. It traces:
1. Direct references in commit messages (e.g., 'fixes #1234')
2. PR-to-issue links via GitHub's API
3. Git trailers (e.g., 'Resolves: JIRA-5678')
4. Branch naming conventions (e.g., 'fix/incident-1234')
5. Temporal clustering (commits within 4 hours of an incident in the same file)
6. LLM-based semantic matching (commit message similarity to incident descriptions)

The CLQS score measures how many of these patterns we can successfully resolve. Supabase scores 94 because they have excellent commit hygiene and issue discipline."

---

### If asked: "Why can't CodeRabbit just add this feature?"

**YOU:** "Three reasons:

1. **Data Moat:** They don't have the incident-linking pipeline. Building Patterns 1-6 and the TreeSitter â†’ Neo4j graph ingestion took us 12 months. They'd need to rebuild all of this.

2. **Pivot Cost:** CodeRabbit is positioned as a 'code quality' tool for reviewers. Asking customers to expose their PagerDuty/Sentry data is a hard pivot and a trust barrier.

3. **Network Effects:** Every incident we ingest improves the model. By the time they launch, we'll have 2 years of incidentâ†’code correlation data across dozens of customers. That's a compounding advantage they can't replicate by cloning our GitHub repo."

---

### If asked: "What if the data is incomplete?"

**YOU:** "That's exactly what the CLQS score solves. If your CLQS is below 70, we'll tell you upfront: 'Your data quality is too poor for accurate risk assessment. Here are the gaps we found.'

This is a feature, not a bug. We're not selling snake oil. If we can't deliver 75%+ accuracy, we won't take your money. This builds trust and ensures our case studies are always credible."

---

### If asked: "How do you avoid false positives?"

**YOU:** "Two mechanisms:

1. **Severity-Weighted Scoring:** Not all risk is equal. A file with 10 LOW-severity incidents scores lower than a file with 1 SEV-1 incident. Our model weighs R_temporal by incident severity.

2. **Continuous Feedback Loop:** When a developer overrides a CRITICAL flag (using `crisk explain --override`), they document why. If the commit is safe, that override trains the model. If it causes an incident, that validates the flag. Over time, the false positive rate decreases asymptotically."

---

## The Leave-Behind Asset

After the meeting, send them a **PDF version of the full audit** with:

1. **Executive Summary (1 page):**
   - The Money Slide graph
   - The 91% accuracy stat
   - The $38.4M ROI calculation

2. **Methodology (1 page):**
   - How you collected the Red Team and Green Team
   - How you computed R_temporal, R_ownership, R_coupling
   - CLQS score breakdown

3. **Detailed Findings (2-3 pages):**
   - Top 10 highest-risk files (with their actual incident histories)
   - Example output of what a developer would have seen
   - Comparison to what the reviewer *actually* saw (just a diff)

4. **Appendix (1 page):**
   - Definitions of R_temporal, R_ownership, R_coupling
   - CLQS scoring rubric
   - References to your economic model paper

**Subject Line:** "Supabase Production Risk Audit - 91% Prediction Accuracy"

This becomes your **single most powerful sales asset**. Every prospect will ask: "Can you do this for us?"

---

## Success Metrics

You know this demo worked if:

- **VCs:** They ask, "What's your data moat?" or "How do you prevent false positives?" (engagement with the substance)
- **Customers:** They ask, "Can you run this on our codebase?" (immediate desire for the audit)
- **CFOs:** They ask, "What's the ROI?" and you can point to the $38.4M slide

If they ask, "How is this different from CodeRabbit?" â€” you've failed. The retrospective audit should make the difference *obvious*.
